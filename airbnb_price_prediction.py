# -*- coding: utf-8 -*-
"""Airbnb_Price_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15I-EpAU2kddY4qTHxY8To-W9FZOT3JMe

# Airbnb Decision Support System (DSS)

This notebook presents a Decision Support System (DSS) for Airbnb hosts, providing price predictions and time series forecasting to optimize pricing and maximize revenue.
"""

#import libraries
import numpy as np
import pandas as pd
import seaborn as sns

"""# EDA

## Data Cleaning
"""

data = pd.read_csv("AB_NYC_2019.csv")
data

data.isnull().sum()

data = data.dropna(subset=["name", "host_name"])
data.isnull().sum()

import matplotlib.pyplot as plt

data.isnull().sum().plot(kind="bar", title="Count of NaN values per column")
plt.show()

data.shape

data.loc[:,"reviews_per_month"] = data["reviews_per_month"].fillna(data["reviews_per_month"].mean())  # Fill with mean
data["reviews_per_month"].isnull().sum()

data.isnull().sum()

data.loc[:,"last_review"].dtype

# Convert the column to datetime
data["last_review"] = pd.to_datetime(data["last_review"], errors="coerce")

# Fill missing values with the most frequent date (mode)
mode_date = data["last_review"].mode()[0]
data["last_review"] = data["last_review"].fillna(mode_date)

data["last_review"].isnull().sum()

data.isnull().sum()

"""### **Key Insights from Data Cleaning**

 **1. General Dataset Overview**
- The dataset contains **48,895 rows** and **16 columns**.
- No **duplicate rows** were found.
- Data types consist of:
  - **Numerical columns:** `int64`, `float64`
  - **Categorical columns:** `object` (text-based features)

---

 **2. Missing Values Analysis**
- **Total missing values:** **20,141**
- Columns with missing values:
  - **`name`**: **16 missing**
  - **`host_name`**: **21 missing**
  - **`last_review`**: **10,052 missing** (likely because some listings never received reviews)
  - **`reviews_per_month`**: **10,052 missing** (corresponds with `last_review`)

---

## Univariate Analysis

Analyze single variables to understand their distributions.

**a. Numerical Features**

Summary statistics:
"""

data.describe()

"""Scatter to visualize outliers:"""

import matplotlib.pyplot as plt

# Scatter plot for price distribution
plt.figure(figsize=(10, 5))
plt.scatter(range(len(data)), data['price'], alpha=0.5, color='blue')
plt.xlabel('Index')
plt.ylabel('Price')
plt.title('Airbnb Price Distribution')
plt.show()

"""ðŸ“Š **Interpretation of the Airbnb Price Distribution Scatter Plot**  

 The scatter plot shows **Airbnb price distribution**, with most listings priced under **$500** but some extreme outliers exceeding **$10,000**. The distribution is **heavily skewed**, suggesting a few luxury or mispriced listings. Prices are scattered across the dataset with no strong pattern. Most listings fall within a predictable range, while outliers may need separate analysis. **Hosts should focus on market trends rather than outliers when pricing listings.** A **log transformation** could help normalize the data for better analysis.

**b. Categorical Features**

Check unique values:
"""

data.columns

data['neighbourhood_group'].value_counts()

"""Visualize with bar plots:"""

sns.countplot(x=data['room_type'])

"""The bar chart shows the distribution of Airbnb listings by room type. **Entire homes/apartments** are the most common, followed by **private rooms**, while **shared rooms** are the least frequent. This suggests that travelers prefer privacy, with entire homes being the top choice. Private rooms remain popular, likely due to affordability. Shared rooms have low demand, possibly appealing to budget travelers. For hosts, listing an **entire home** offers the highest market exposure, while private rooms are also a strong option. Shared rooms may require competitive pricing to attract guests.

## Bivariate & Multivariate Analysis

Analyze relationships between variables.

**a. Price Analysis**

Price distribution per neighborhood:
"""

# Price distribution per neighborhood:
sns.barplot(x='neighbourhood_group', y='price', data=data)

"""This bar chart compares the average price of listings across different neighborhood groups in New York City. Manhattan has the highest average price, followed by Brooklyn and Staten Island, while Queens and the Bronx have lower prices. The error bars indicate the variability in pricing, with Staten Island showing the highest variation."""

#Price vs. room type:
sns.barplot(x='room_type', y='price', data=data)

"""This bar chart compares the average price of different room types. Entire homes/apartments have the highest average price, followed by private rooms, while shared rooms are the cheapest. The error bars indicate variability, with entire homes having the least uncertainty and shared rooms showing slightly more variation.

**b. Correlations**

Check correlation matrix:
"""

import pandas as pd

# Convert 'last_review' to a proper datetime format instead of dropping it
data_temp = data.drop(columns=['name', 'host_name', 'neighbourhood'])
data_temp['last_review'] = pd.to_datetime(data['last_review'])

# Encode categorical columns but KEEP 'neighbourhood_group' and 'last_review'
categorical_cols = ['room_type','neighbourhood_group', ]  # Specify categorical columns to encode
data_encoded = pd.get_dummies(data_temp, columns=categorical_cols, drop_first=True)

print(data_encoded.info())

#Heatmap:
plt.figure(figsize=(12, 8))
sns.heatmap(data_encoded.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Matrix (Including Categorical Variables)")
plt.show()

"""With respect to **price**, the correlation matrix shows the following insights:  

- **Price has a weak correlation with most variables**, meaning no single feature strongly predicts price.  
- **Neighbourhood Group (Manhattan) has a slight positive correlation (~0.16)**, indicating prices tend to be higher in Manhattan.  
- **Room Type (Entire Home/Apt) likely has a positive correlation with price**, though it is not explicitly shown in the matrix. Private and shared rooms generally have negative or weak correlations.  
- **Minimum Nights has a weak negative correlation (~-0.15)**, suggesting that longer minimum stays slightly lower the price.  
- **Other features like number of reviews and availability show little to no impact on price.**  

Overall, price is primarily influenced by location and room type, with other variables playing a minor role.

**c. Availability & Price Relationship**

Line plot:
"""

sns.lineplot(x='availability_365', y='price', data=data)

"""The plot shows the relationship between **availability_365** (days available per year) and **price**. Prices vary widely at all availability levels, with some high-price outliers. Listings with **low availability** may be seasonal or luxury properties, while **high-availability** listings tend to stabilize in price but still show fluctuations. There is **no clear correlation**, suggesting other factors (e.g., location, amenities) influence pricing more. Hosts should consider **demand trends** rather than just availability when setting prices.

## Geospatial Analysis
If location data (latitude & longitude) is available:
"""

# Use folium for interactive maps:
import folium
from folium.plugins import HeatMap

m = folium.Map(location=[data['latitude'].mean(), data['longitude'].mean()], zoom_start=12)
HeatMap(data=data[['latitude', 'longitude', 'price']].values, radius=15).add_to(m)
m

"""This is a heatmap representing Airbnb listing density in New York City. The color gradient ranges from blue (low density) to red (high density). The highest concentrations of listings are in Manhattan and parts of Brooklyn, as indicated by the red and yellow areas. Other boroughs, like Staten Island and the Bronx, show lower listing densities with blue and green areas. This visualization highlights popular Airbnb hotspots, likely corresponding to major tourist and commercial areas.

# Training and Evaluation the Price Model
"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_absolute_error

# Data Preprocessing
clean_data = data[data['price'] > 0]  # Remove entries with zero price

# Encode categorical variables
le = LabelEncoder()
clean_data['neighbourhood_group'] = le.fit_transform(clean_data['neighbourhood_group'])
clean_data['room_type'] = le.fit_transform(clean_data['room_type'])

# Feature Selection
features = [
    'neighbourhood_group', 'latitude', 'longitude',
    'room_type', 'minimum_nights', 'number_of_reviews',
    'reviews_per_month', 'calculated_host_listings_count',
    'availability_365'
]

X = clean_data[features]
y = clean_data['price']

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

le = LabelEncoder()
y_train_encoded = le.fit_transform(y_train)
y_test_encoded = le.fit_transform(y_test)  # Ensure test labels exist in training

# Model Training
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train_encoded)

# Prediction and Evaluation
y_pred = model.predict(X_test)
mae = mean_absolute_error(y_test_encoded, y_pred)

print(f"Mean Absolute Error: {mae:.2f}")

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor

# Define target and features
X = clean_data[features]
y = clean_data['price']

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

le = LabelEncoder()
y_train_encoded = le.fit_transform(y_train)
y_test_encoded = le.fit_transform(y_test)  # Ensure test labels exist in training

# Define models
models = {
    "Linear Regression": LinearRegression(),
    "Decision Tree": DecisionTreeRegressor(max_depth=5),
    "Random Forest": RandomForestRegressor(n_estimators=100),
    "XGBoost": XGBRegressor(n_estimators=100)
}

# Dictionary to store results
results = {}

# Evaluate all models
for name, model in models.items():
    print(f"Training {name}...")
    model.fit(X_train, y_train_encoded)
    y_pred = model.predict(X_test)

    # Compute Metrics
    mae = mean_absolute_error(y_test_encoded, y_pred)
    mse = mean_squared_error(y_test_encoded, y_pred)
    rmse = rmse = np.sqrt(mse)
    r2 = r2_score(y_test_encoded, y_pred)
    cv_rmse = -cross_val_score(model, X, y, scoring='neg_root_mean_squared_error', cv=5).mean()

    # Store results
    results[name] = {"MAE": mae, "MSE": mse, "RMSE": rmse, "RÂ²": r2, "CV RMSE": cv_rmse}

# Convert results to DataFrame
results_df = pd.DataFrame(results).T
print("\nModel Evaluation Results:")
print(results_df)

# Plot Residuals for the Best Model
best_model_name = results_df["RMSE"].idxmin()
best_model = models[best_model_name]
y_pred_best = best_model.predict(X_test)

residuals = y_test_encoded - y_pred_best
sns.histplot(residuals, bins=30, kde=True)
plt.xlabel("Residuals")
plt.title(f"Residual Distribution ({best_model_name})")
plt.show()

"""#### **Summary & Interpretation of Model Performance**  

##### **Residual Distribution (XGBoost)**
- The histogram shows that the residuals (prediction errors) are **approximately normally distributed** with a mean around **zero**, indicating that the XGBoost model does not have a strong bias.
- The spread of residuals suggests that most predictions are close to actual values, but there are some extreme values.

#### **Final Interpretation**
- **XGBoost outperforms all models** across most metrics, making it the best choice.
- **Linear Regression performs the worst**, likely due to non-linearity in the data.
- **Decision Tree improves over Linear Regression**, but **Random Forest and XGBoost** handle complexity better.
- **XGBoost should be preferred** for Airbnb price prediction, but further fine-tuning might improve results.

## Testing the model
"""

# Rename columns before concatenation
clean_data = clean_data.rename(columns={"room_type": "room_type_encoded", "neighbourhood_group": "neighbourhood_group_encoded"})

# Concatenate the modified DataFrames
concatenated_df = pd.concat([
    clean_data["room_type_encoded"],
    clean_data["neighbourhood_group_encoded"],
    data["room_type"],
    data["neighbourhood_group"]
], axis=1)

# Display the result
#concatenated_df.head(5)

# Group by neighbourhood and neighbourhood encoded
neighbourhood_grouped_df = concatenated_df.groupby("neighbourhood_group")["neighbourhood_group_encoded"].mean()
neighbourhood_grouped_df

# Group by room_type and room_type encoded
room_type_df = concatenated_df.groupby("room_type")["room_type_encoded"].mean()
room_type_df

# DSS Function for Price Suggestion
def suggest_price(listing_details):
    input_data = pd.DataFrame([listing_details])
    input_data['neighbourhood_group'] = le.fit_transform(input_data['neighbourhood_group'])
    input_data['room_type'] = le.fit_transform(input_data['room_type'])

    suggested_price = best_model.predict(input_data)[0]
    return round(suggested_price, 2)

data.head(1)

# Example 1 Usage
example_listing = {
    'neighbourhood_group': 1,
    'latitude': 40.64749,
    'longitude': -73.97237,
    'room_type': 1,
    'minimum_nights': 1,
    'number_of_reviews': 9,
    'reviews_per_month': 0.21,
    'calculated_host_listings_count': 6,
    'availability_365': 365
}

print(f"Suggested Price: ${suggest_price(example_listing)}")

# Example 2 Usage
example_listing = {
    'neighbourhood_group': 2,
    'latitude': 40.748817,
    'longitude': -73.985428,
    'room_type': 1,
    'minimum_nights': 2,
    'number_of_reviews': 50,
    'reviews_per_month': 1.5,
    'calculated_host_listings_count': 1,
    'availability_365': 200
}

print(f"Suggested Price: ${suggest_price(example_listing)}")

"""## Saving the best model"""

import pickle

with open("best_price_model.pkl", "wb") as file:
    pickle.dump(best_model, file)